# ğŸ§  Agentic AI System for Multi-Step Tasks

An end-to-end **agentic AI system** that decomposes complex user tasks into multiple steps and coordinates specialized agents using **async pipelines** and **Redis-based message queues**.

This project demonstrates **agent boundaries, orchestration, streaming responses, failure handling, and scalability**, strictly aligned with typical agentic-system assignment requirements.

---

## ğŸ“Œ Problem Statement

Design and implement an **Agentic AI System** capable of:

- Accepting a complex user task  
- Breaking it into multiple steps  
- Assigning steps to specialized agents  
- Coordinating execution asynchronously  
- Streaming partial responses to the client  
- Handling retries, failures, and scalability concerns  

---

## ğŸ§© System Overview

### High-Level Flow

```
Client (FastAPI)
   |
   v
Orchestrator
   |
   v
Task Planner Agent
   |
   v
Redis Queue (step-wise routing)
   |
   +--> Retriever Agent
   +--> Analyzer Agent
   +--> Writer Agent
   |
   v
Final Result â†’ Streamed to Client
```

---

## ğŸ¤– Agents Implemented

| Agent | Responsibility |
|-----|---------------|
| Task Planner | Decomposes user input into ordered steps |
| Retriever | Gathers factual / contextual information |
| Analyzer | Performs reasoning and analysis |
| Writer | Produces final structured output |
| Orchestrator | Routes tasks, manages state, handles failures |

Each agent:

- Runs independently  
- Consumes tasks from Redis  
- Publishes results back to Redis  
- Uses async processing  

---

## ğŸ§± Architecture Decisions

### Why Agent-Based?
- Clear separation of concerns  
- Horizontal scalability  
- Easier debugging and observability  

### Why Redis?
- Lightweight message queue  
- Async-friendly  
- Easy local + cloud deployment  
- Supports pub/sub and task queues  

### Why Async (asyncio)?
- Non-blocking I/O  
- Concurrent agent execution  
- Scalable orchestration  

---

## ğŸ“‚ Project Structure

```
agentic_system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ planner_agent.py
â”‚   â”‚   â”œâ”€â”€ retriever_agent.py
â”‚   â”‚   â”œâ”€â”€ analyzer_agent.py
â”‚   â”‚   â””â”€â”€ writer_agent.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â”œâ”€â”€ redis_client.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ” Async Orchestration Flow

1. Client submits a task via API  
2. Orchestrator sends task to Planner  
3. Planner returns structured steps  
4. Orchestrator dispatches each step to the correct agent  
5. Agents return results asynchronously  
6. Results are streamed to client using Server-Sent Events  
7. Final output is stored and returned  

---

## ğŸ”„ Streaming Responses

- Implemented using **FastAPI + Server-Sent Events (SSE)**
- Client receives:
  - Processing updates  
  - Final completed result  

This keeps long-running tasks responsive.

---

## ğŸ§ª Mock Mode vs Real Gemini API

### Why Mock Mode Exists
Due to Gemini free-tier quota exhaustion, the system supports a **Mock Mode** to:

- Demonstrate full orchestration  
- Validate agent communication  
- Show streaming and retries  
- Enable reliable live demo during evaluation  

### Mock Mode (Default)
```
MOCK_MODE=true
```

### Switching to Gemini API
```
MOCK_MODE=false
GEMINI_API_KEY=your_api_key_here
```

> No code changes required â€” only `.env` update.

---

## ğŸš€ Running the System

### 1ï¸âƒ£ Start Redis
```
docker run -d -p 6379:6379 redis:7
```

### 2ï¸âƒ£ Start Agents (separate terminals)
```
python -m app.agents.planner_agent
python -m app.agents.retriever_agent
python -m app.agents.analyzer_agent
python -m app.agents.writer_agent
```

### 3ï¸âƒ£ Start Orchestrator
```
python -m app.orchestrator
```

### 4ï¸âƒ£ Start API
```
uvicorn app.main:app --reload
```

---

## ğŸ“¡ API Usage

### Submit Task
```
POST /task
{
  "user_input": "Research 2026 solid-state battery technology and summarize for investors"
}
```

### Stream Results
```
GET /stream/{task_id}
```

---

## âš ï¸ Failure Handling

- Agent-level failure propagation  
- Retry logic with exponential backoff  
- Graceful task termination  
- Final error streamed to client  

---

## ğŸ“ˆ Scalability Considerations

- Stateless agents  
- Redis-based task routing  
- Horizontal scaling via multiple agent instances  
- Clear agent boundaries  

---

## ğŸ“ Post-Mortem Summary

### Scaling Issue Encountered
- Gemini API free-tier rate limits caused planner failures  

### Design Decision
- Introduced request batching + shared embedding cache  

### Trade-offs Made
- Mock Mode used for demo stability  
- Redis chosen over Kafka for simplicity  

---

âœ… **This project showcases real-world agentic AI system design with orchestration, async execution, and streaming.**
